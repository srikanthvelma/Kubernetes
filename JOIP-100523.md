#### 1.Create 1 master node and 2 worker nodes â€“ run app on node1 and db on node2 by using 
#### a.	Node selector 
#### b.	Affinity 
#### c.	Taints and tolerances 

* creating aks cluster with 2 nodes
* added labels for nodes as node1 and node2
![preview](images/k8jp11_01.png)
![preview](images/k8jp11_02.png)
* write manifest files for one app to select `node1`
* and one db to select `node2`
![preview](images/k8jp11_03.png)
* nop
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nopcommerce-deploy
spec:
  minReadySeconds: 60
  replicas: 1
  selector:
    matchLabels:
      app: nop
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      name: nop-pod
      labels:
        app: nop
    spec:
      nodeSelector:
        nodename: node1
      containers:
        - name: nop-ctnr
          image: srikanthvelma/nopcommerce:v1.0
          ports:
            - containerPort: 5000
              protocol: TCP
          env:
            - name: ConnectionStrings__WebAppContext
              value: "server=mysql-svc;uid=root;pwd=rootroot;database=employees"
            - name: MYSQL_SERVER
              value: mysql-stateful-0.mysql-svc
```
* mysql
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deploy
spec:
  minReadySeconds: 5
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      name: mysql-pod
      labels:
        app: mysql
    spec:
      nodeSelector:
        nodename: node2
      containers:
        - name: mysql-ctnr
          image: mysql:8
          ports:
            - containerPort: 3306
          envFrom:
            - configMapRef:
                name: mysql-configmap
                optional: false  
```
#### Affinity
![preview](images/k8jp11_04.png)
* nop
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nopcommerce-deploy
spec:
  minReadySeconds: 60
  replicas: 1
  selector:
    matchLabels:
      app: nop
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      name: nop-pod
      labels:
        app: nop
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nodename
                    operator: In
                    values: 
                      - node1
      containers:
        - name: nop-ctnr
          image: srikanthvelma/nopcommerce:v1.0
          ports:
            - containerPort: 5000
              protocol: TCP
          env:
            - name: ConnectionStrings__WebAppContext
              value: "server=mysql-svc;uid=root;pwd=rootroot;database=employees"
            - name: MYSQL_SERVER
              value: mysql-stateful-0.mysql-svc
```
* mysql
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deploy
spec:
  minReadySeconds: 5
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      name: mysql-pod
      labels:
        app: mysql
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nodename
                    operator: In
                    values: 
                      - node2
      containers:
        - name: mysql-ctnr
          image: mysql:8
          ports:
            - containerPort: 3306
          envFrom:
            - configMapRef:
                name: mysql-configmap
                optional: false
```
#### Taints and tolerences
* first we have to attach taints to nodes
* `kubectl taint nodes <node-name> <key=value:effect>`
```sh
kubectl taint nodes aks-node-65756001 app=db:NoSchedule
kubectl taint nodes aks-node-65756002 app=web:NoSchedule
```
* create a manifest by writing tolerences in it
* here taints are opposite to selector, if we observe in above example
* if tolerence of app=db matches - it will not schedule in this machine
* nop
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nopcommerce-deploy
spec:
  minReadySeconds: 60
  replicas: 1
  selector:
    matchLabels:
      app: nop
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      name: nop-pod
      labels:
        app: nop
    spec:
      tolerations:
        - key: app
          operator: Equal
          value: db
          effect: NoSchedule
      containers:
        - name: nop-ctnr
          image: srikanthvelma/nopcommerce:v1.0
          ports:
            - containerPort: 5000
              protocol: TCP
          env:
            - name: ConnectionStrings__WebAppContext
              value: "server=mysql-svc;uid=root;pwd=rootroot;database=employees"
            - name: MYSQL_SERVER
              value: mysql-stateful-0.mysql-svc
```
* mysql
```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deploy
spec:
  minReadySeconds: 5
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      name: mysql-pod
      labels:
        app: mysql
    spec:
      tolerations:
        - key: app
          operator: Equal
          value: nop
          effect: NoSchedule
      containers:
        - name: mysql-ctnr
          image: mysql:8
          ports:
            - containerPort: 3306
          envFrom:
            - configMapRef:
                name: mysql-configmap
                optional: false
```
![preview](images/k8jp11_05.png)

## Upgrade Kubernetes cluster from version 1.25 to version 1.27
* we have to upgrade to 1.26 first then 1.27
* Created a kubernetes cluster with kubeadm
  * masternode
  * 2 worker nodes
* Here i have run all commands of kubeadm process with few changes
* changes are
* installing `kubeadm`, `kubectl`, `kubelet` with specific versions
```sh
sudo apt install kubeadm=1.25.9-00 kubectl=1.25.9-00 kubelet=1.25.9-00 -y
``` 
![preview](images/k8jp11_06.png)
* and installing kubernetes with version
```sh
 kubeadm init --kubernetes-version "1.25.9" --pod-network-cidr "10.244.0.0/16" --cri-socket unix:///var/run/cri-dockerd.sock
```
![preview](images/k8jp11_07.png)
* After doing the control plane creation and nodes join so successfully installed k8s with v 1.25
![preview](images/k8jp11_08.png)
* for testing i have ran 2 pods in each in one node
![preview](images/k8jp11_09.png) 

### Upgrading Workflow
* upgrade primary control plane node
* upgrade additional control plane nodes
* upgrade worker nodes

#### upgrading control plane node - Masre node
* Steps
  * Upgrade kubeadm on the Control Plane node
  * Drain the Control Plane node
  * Plan the upgrade (kubeadm upgrade plan)
  * Apply the upgrade (kubeadm upgrade apply)
  * Upgrade kubelet & kubectl on the control Plane node
  * Uncordon the Control Plane node
##### 1. upgrade control plane node
* Determine which version to upgrade
```sh
apt update
apt-cache madison kubeadm
```
![preview](images/k8jp11_10.png)
* upgrade plan
```sh
kubeadm upgrade plan
```
![preview](images/k8jp11_11.png)
* so now updrading kubeadm tool
```sh
apt-mark unhold kubeadm
apt-get update
apt-get install kubeadm=1.26.0-00
apt-mark hold kubeadm
```
![preview](images/k8jp11_12.png)
![preview](images/k8jp11_13.png)
![preview](images/k8jp11_14.png)
* upgrade plan and apply new version
```sh
kubeadm upgrade plan
kubeadm upgrade apply v1.26.0
```
![preview](images/k8jp11_15.png)
![preview](images/k8jp11_16.png)
![preview](images/k8jp11_17.png)
* uncordon control node
```sh
 kubectl uncordon kube-adm
```
* next upgrade kubectl and kubelet
* after that daemon reload and restart kubectl
```sh
apt-mark unhold kubectl kubelet
apt update
apt install kubectl=1.26.0-00 kubelet=1.26.0-00 -y
apt-mark hold kubectl kubelet
systemctl daemon-reload
systemctl restart kubectl
```
![preview](images/k8jp11_18.png)
![preview](images/k8jp11_19.png)
* now master node successfully upgraded to 1.26.0
![preview](images/k8jp11_20.png)

### upgrading worker nodes
* steps
  * unhold kudeadm and install req version and then hold
  * Drain worker node from `master node`
  * upgrade kudeadm on worker node
  * unhold kubectl and kubelet and instal req version and then hold
  * Restart `kubelet`
  * uncordon worker node from `master node`
* **KUBEADM**
* run following on worker nodes
```sh
apt-mark unhold kubeadm
apt-get update
apt-get install kubeadm=1.26.0-00 -y
apt-mark hold kubeadm
```
![preview](images/k8jp11_21.png)
* drain nodes from master node
```sh
kubectl drain <node name> <args>
kubectl drain kubenode-1 --ignore-daemonsets
kubectl drain kubenode-2 --ignore-daemonsets
```
![preview](images/k8jp11_22.png)
![preview](images/k8jp11_23.png)
* upgrade node run command from node only
```sh
kubeadm upgrade node
```
![preview](images/k8jp11_24.png)
* **KUBECTL & KUBELET**
* now upgrade kubelet and kubectl
```sh
apt-mark unhold kubectl kubelet
apt-get update
apt-get install kubectl=1.26.0-00 kubelet=1.26.0-00 -y
apt-mark hold kubectl kubelet
```
![preview](images/k8jp11_25.png)
* restart kubelet
```sh
systemctl daemon-reload
systemctl restart kubelet
```
![preview](images/k8jp11_27.png)
* uncordon nodes from `master node`
```sh
kubectl uncordon <node name>
```
![preview](images/k8jp11_28.png)
* Now check status of nodes and pods
![preview](images/k8jp11_29.png)
* so now cluster is successfully upgraded to 1.26.0
### Upgrading to 1.26.0 to 1.27.1
* we have to follow above steps
